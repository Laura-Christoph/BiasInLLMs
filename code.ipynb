{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the format of the tsv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "external_disk = \"*\"\n",
    "input_file_23 = os.path.join(external_disk, \"processed_reddit_posts_2024-03.tsv\")\n",
    "input_file_24 = os.path.join(external_disk, \"2024_03_preprocessed.tsv\")\n",
    "input_file_24_dedup = os.path.join(external_disk, \"2023_02_prepro_train_anonym_V2.tsv\")\n",
    "\n",
    "def read_and_print_lines(file_path, num_lines=50):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        count = 0\n",
    "        while count < num_lines:\n",
    "            line = f.readline()\n",
    "            if not line:  # Stopping if we reach the end of the file\n",
    "                break\n",
    "            # Splitting the line by tabs and print the resulting list\n",
    "            print(line.strip().split(\"\\t\"))\n",
    "            count += 1\n",
    "\n",
    "# Processing the first 10 lines of each file\n",
    "\"\"\"\n",
    "print(\"First 10 lines of 2023_02_preprocessed.tsv:\")\n",
    "read_and_print_lines(input_file_23)\n",
    "\n",
    "print(\"\\nFirst 10 lines of 2024_03_preprocessed.tsv:\")\n",
    "read_and_print_lines(input_file_24)\"\"\"\n",
    "print(\"\\nFirst 10 lines of 2024_03_preprocessed.tsv:\")\n",
    "read_and_print_lines(input_file_24_dedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making one column with the text data, meaning title and content combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "external_disk = \"*\"\n",
    "input_file_23 = os.path.join(external_disk, \"processed_reddit_posts_2023-02.tsv\")\n",
    "input_file_24 = os.path.join(external_disk, \"processed_reddit_posts_2024-03.tsv\")\n",
    "output_file_23 = os.path.join(external_disk, \"2023_02_preprocessed.tsv\")\n",
    "output_file_24 = os.path.join(external_disk, \"2024_03_preprocessed.tsv\")\n",
    "\n",
    "headers = ['author', 'category', 'content_categories', 'created_time', 'discussion_type', 'id', 'over_18', 'whitelist_status', 'removed_by_category', 'selftext', 'subreddit', 'subreddit_id', 'subreddit_subscribers', 'subreddit_type', 'title', 'is_self']\n",
    "\n",
    "def process_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Writing the headers with the new 'text' column\n",
    "        headers_with_text = headers.copy()\n",
    "        headers_with_text.insert(headers.index('created_time'), 'text')\n",
    "        outfile.write('\\t'.join(headers_with_text) + '\\n')\n",
    "        \n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == len(headers):\n",
    "                # Extracting necessary fields\n",
    "                title = parts[headers.index('title')]\n",
    "                selftext = parts[headers.index('selftext')]\n",
    "                is_self = parts[headers.index('is_self')]\n",
    "                \n",
    "                # Creating the 'text' column\n",
    "                text = f\"{title}: {selftext}\" if is_self.lower() == 'true' else title\n",
    "                \n",
    "                # Inserting the 'text' column in the correct position\n",
    "                parts.insert(headers.index('created_time'), text)\n",
    "                \n",
    "                # Writing the modified line to the output file\n",
    "                outfile.write('\\t'.join(parts) + '\\n')\n",
    "\n",
    "# Processing both files\n",
    "process_file(input_file_23, output_file_23)\n",
    "process_file(input_file_24, output_file_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text data to recognize spam better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "external_disk = \"*\"\n",
    "input_file_24 = f\"{external_disk}/2024_03_preprocessed.tsv\"\n",
    "output_file_24 = f\"{external_disk}/2024_03_prepro_cleaned.tsv\"\n",
    "\n",
    "# Improved regex for repeated punctuation and emojis\n",
    "repeat_pattern = regex.compile(r\"([\\p{P}\\p{So}])\\1+\", flags=regex.UNICODE)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing repeated visible emojis or punctuation\n",
    "    text = repeat_pattern.sub(r\"\\1\", text)  \n",
    "\n",
    "    # Normalizing Unicode characters (fixes invisible spaces)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Removing completely invisible characters (except spaces)\n",
    "    text = \"\".join(char for char in text if char.isprintable() or char.isspace())\n",
    "\n",
    "    return text\n",
    "\n",
    "with open(input_file_24, \"r\", encoding=\"utf-8\") as infile, open(output_file_24, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        columns = line.strip().split(\"\\t\")  # Split the line into columns\n",
    "        if len(columns) >= 4:  # Ensure the line has at least 4 columns\n",
    "            columns[3] = clean_text(columns[3])  # Clean the 4th column (text column)\n",
    "        outfile.write(\"\\t\".join(columns) + \"\\n\")  # Write the modified line to the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training data\n",
    "\n",
    "### Anonymize unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths\n",
    "external_disk = \"*\"\n",
    "input_file_23 = f\"{external_disk}/2023_02_prepro_train_anonym.tsv\"\n",
    "output_file_23 = f\"{external_disk}/2023_02_prepro_train_anonym_V2.tsv\"\n",
    "\n",
    "input_file_24 = f\"{external_disk}/2024_03_prepro_unique.tsv\"\n",
    "output_file_24 = f\"{external_disk}/2024_03_prepro_train_anonym.tsv\"\n",
    "\n",
    "def anonymize_keep_fourth_column(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processes a large TSV file line by line, replacing all values except the fourth column with \"None\".\n",
    "    Ensures all rows have the same number of columns as the header by padding shorter rows.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input TSV file.\n",
    "        output_file (str): Path to save the anonymized TSV file.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        # Reading the header and determine the expected column count\n",
    "        header = infile.readline().strip().split(\"\\t\")  # Reading header line\n",
    "        num_columns = len(header)  # Total number of columns\n",
    "        print(\"Header:\", header)\n",
    "        print(f\"Expected number of columns: {num_columns}\")\n",
    "\n",
    "        outfile.write(\"\\t\".join(header) + \"\\n\")  # Writing header to output\n",
    "\n",
    "        # Processing each line\n",
    "        for line_num, line in enumerate(infile, start=2):  # Starting at line 2 (after header)\n",
    "            fields = line.strip().split(\"\\t\")  # Splitting line into columns\n",
    "\n",
    "            # Ensuring there are at least 4 columns before accessing fields[3]\n",
    "            if len(fields) < 4:\n",
    "                print(f\"Warning: Line {line_num} has less than 4 columns. Fixing it.\")\n",
    "                fields += [\"None\"] * (4 - len(fields))  # Ensure at least 4 columns\n",
    "\n",
    "            # Keeping only column 4 (index 3), replacing others with \"None\"\n",
    "            anonymized_fields = [\"None\"] * num_columns  # Default to \"None\"\n",
    "            anonymized_fields[3] = fields[3]  # Keep original fourth column\n",
    "\n",
    "            # Ensuring correct number of columns\n",
    "            if len(anonymized_fields) < num_columns:\n",
    "                anonymized_fields += [\"None\"] * (num_columns - len(anonymized_fields))\n",
    "\n",
    "            outfile.write(\"\\t\".join(anonymized_fields) + \"\\n\")  # Write modified line to output\n",
    "\n",
    "    print(f\" Processing complete. Anonymized file saved as {output_file}.\")\n",
    "\n",
    "\n",
    "# Defining paths\n",
    "external_disk = \"*\"\n",
    "input_file_24 = f\"{external_disk}/2024_03_prepro_unique.tsv\"\n",
    "output_file_24 = f\"{external_disk}/2024_03_prepro_train_anonym.tsv\"\n",
    "\n",
    "# Running anonymization\n",
    "# anonymize_keep_fourth_column(input_file_24, output_file_24)\n",
    "anonymize_keep_fourth_column(input_file_23, output_file_23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all duplicate lines\n",
    "using the unix commands to deduplicate more efficiently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Defining paths\n",
    "external_disk = \"*\"\n",
    "input_file_23 = f\"{external_disk}/2023_02_prepro_train_anonym.tsv\"\n",
    "output_file_23 = f\"{external_disk}/2023_02_prepro_unique.tsv\"\n",
    "\n",
    "input_file_24 = f\"{external_disk}/2024_03_prepro_train_anonym.tsv\"\n",
    "output_file_24 = f\"{external_disk}/2024_03_prepro_unique.tsv\"\n",
    "\n",
    "# Defining the Unix command for deduplication (keeping only unique lines)\n",
    "command_23 = f'awk -F\\'\\\\t\\' \\'!seen[$0]++\\' \"{input_file_23}\" > \"{output_file_23}\"'\n",
    "command_24 = f'awk -F\\'\\\\t\\' \\'!seen[$0]++\\' \"{input_file_24}\" > \"{output_file_24}\"'\n",
    "\n",
    "# Function to run the deduplication command\n",
    "def deduplicate_file(command, output_file):\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(f\"Successfully deduplicated file. Output saved to {output_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred while running the command: {e}\")\n",
    "\n",
    "# Running deduplication for both files\n",
    "#deduplicate_file(command_23, output_file_23)\n",
    "deduplicate_file(command_24, output_file_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering posts that contain potentially religious terms and filtering out common misclassified keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining key words\n",
    "religious_keywords ={\n",
    "    # General Religious Terms\n",
    "    \"religion\", \"faith\", \"spirituality\", \"sacred\", \"divine\", \"worship\", \"prayer\", \"belief\", \"holy\", \"doctrine\",\n",
    "    \"scripture\", \"theology\", \"sermon\", \"evangelism\", \"clergy\",\n",
    "\n",
    "    #  Christianity-Related Terms\n",
    "    \"Christian\", \"Bible\", \"Jesus\", \"Christ\", \"cross\", \"church\", \"pastor\", \"priest\", \"gospel\", \"crucifix\",\n",
    "    \"resurrection\", \"sin\", \"salvation\", \"pope\", \"Vatican\", \"Protestant\", \"Catholic\", \"baptist\", \"Jehovah\",\n",
    "    \"LDS\", \"Mormon\", \"evangelist\",\n",
    "\n",
    "    #  Islam-Related Terms\n",
    "    \"Islam\", \"Muslim\", \"Quran\", \"Muhammad\", \"mosque\", \"jihad\", \"sharia\", \"hadith\", \"halal\", \"haram\",\n",
    "    \"imam\", \"caliph\", \"Ummah\", \"Mecca\", \"hijab\", \"niqab\", \"burqa\", \"fatwa\",\n",
    "\n",
    "    #  Judaism-Related Terms\n",
    "    \"Jewish\", \"Judaism\", \"Torah\", \"Talmud\", \"synagogue\", \"rabbi\", \"Zionism\", \"anti-semitic\", \"Israel\",\n",
    "    \"kabbalah\", \"kosher\", \"mitzvah\", \"Hanukkah\", \"Passover\", \"Yom Kippur\", \"shabbat\",\n",
    "\n",
    "    #  Hinduism & Other Religions\n",
    "    \"Hindu\", \"Hinduism\", \"temple\", \"Vedas\", \"karma\", \"dharma\", \"reincarnation\", \"moksha\", \"puja\", \"Shiva\",\n",
    "    \"Vishnu\", \"Brahma\", \"mantra\", \"Upanishads\", \"Buddhism\", \"Sikh\", \"Jain\", \"guru\", \"monk\", \"Nirvana\",\n",
    "\n",
    "    #  Atheism, Agnosticism & Anti-Religious Terms\n",
    "    \"atheist\", \"agnostic\", \"secular\", \"irreligious\", \"blasphemy\", \"anti-theist\", \"godless\", \"heresy\",\n",
    "    \"apostate\", \"unbeliever\", \"heathen\", \"infidel\",\n",
    "\n",
    "    #  Religious Hate Speech & Extremism Keywords\n",
    "    \"religious war\", \"religious hate\", \"religious extremism\", \"fundamentalist\", \"infidels\", \"crusade\",\n",
    "    \"inquisition\", \"jihadist\", \"radical Islam\", \"Christian nationalism\", \"terrorist\", \"suicide bomber\",\n",
    "    \"pagan\", \"devil worship\", \"satanic\", \"cult\", \"brainwashing\", \"false prophet\", \"religious oppression\",\n",
    "    \"forced conversion\", \"religious discrimination\", \n",
    "\n",
    "    #   Newly Added Religious Hate Speech Terms  \n",
    "    \"heretic\", \"apostate\", \"blasphemer\", \"false religion\", \"fake religion\", \"religious scam\",\n",
    "    \"religious brainwashing\", \"religious cult\", \"burn in hell\", \"godless heathens\",\n",
    "    \"convert or die\", \"holy war\", \"purge the unbelievers\", \"wipe them out\", \"death to infidels\",\n",
    "    \"crusader scum\", \"jihadi terrorists\", \"sharia law takeover\", \"Zionist conspiracy\",\n",
    "    \"Christian supremacy\", \"Muslim invasion\", \"Jewish globalists\", \"Satanic agenda\",\n",
    "    \"Islamophobic\", \"Christophobic\", \"anti-Semitic\", \"religious genocide\", \"religious cleansing\",\n",
    "    \"ban religious teachings\", \"religious fundamentalism\", \"satanic ritual\", \"burn the Qur'an\",\n",
    "    \"ban the Bible\", \"ban the Torah\", \"ban the Quran\", \"religious dictatorship\",\n",
    "    \"anti-religious propaganda\", \"forced religious conversion\", \"eradicate non-believers\",\n",
    "    \"destroy Christianity\", \"destroy Islam\", \"destroy Judaism\", \"destroy Hinduism\",\n",
    "    \"ban all religious practices\", \"no room for religion\", \"deport all Muslims\",\n",
    "    \"Jewish banking conspiracy\", \"Hindus are extremists\", \"violent Christian evangelicals\",\n",
    "    \"Muslim rape gangs\", \"Islam is terrorism\", \"Jews control the world\", \"atheists are evil\",\n",
    "    \"Satan controls religion\", \"God hates [religion]\", \"God's punishment for [group]\",\n",
    "    \"God will judge them\", \"devil worshipers\", \"enemy of God\", \"infidel punishment\",\n",
    "    \"forced religious laws\", \"hate the infidels\", \"no freedom for religion\", \"death to heretics\",\n",
    "    \"Christians are oppressors\", \"Islam is a cult\", \"Jews are parasites\", \"Muslims are terrorists\",\n",
    "    \"burn the churches\", \"destroy all mosques\", \"synagogues are evil\", \"ban Hindu temples\",\n",
    "    \"burn them at the stake\", \"witch hunt\", \"spiritual corruption\", \"destroy their beliefs\",\n",
    "    \"no place for religion in society\", \"eradicate the religious\", \"religion is slavery\",\n",
    "    \"atheist dictatorship\", \"God is a lie\", \"crusaders vs jihadists\", \"stop Islamic expansion\",\n",
    "    \"stop Christian imperialism\", \"ban all religious books\", \"the true religion vs fake religion\",\n",
    "    \"dismantle religious power\", \"kill in God's name\", \"religious persecution\", \"religious fanatics\",\n",
    "    \"purge the heathens\", \"holy war against [religion]\", \"destroy their idols\", \"no God allowed\",\n",
    "    \"faith-based terror\", \"Christian jihad\", \"Muslim crusaders\", \"radical Judaism\", \"Hindu terrorism\",\n",
    "    # abbreviations\n",
    "     #  General Religious Hate Speech Abbreviations\n",
    "    \"rwds\", \"kti\", \"fbi\", \"kek\", \"nsm\", \"oy vey\", \"1488\", \"14w\", \"rahowa\", \"zog\", \"zio\",\n",
    "    \"cult\", \"sjw\", \"lsh\", \"kafir\", \"taqiyya\", \"isis\", \"ds\", \"npc\",\n",
    "\n",
    "    #  Hate Groups & Extremism Abbreviations\n",
    "    \"kkk\", \"wbc\", \"nifb\", \"cwp\", \"ci\", \"wwg1wga\", \"ie\",\n",
    "\n",
    "    #  Online Slang & Coded Religious Hate Speech\n",
    "    \"larp\", \"boomer\", \"groy\", \"sh\", \"atheism+\", \"gen zion\", \"mt\", \"cuck\", \"christcuck\",\n",
    "    \"jidf\", \"hv\", \"666\"\n",
    "}\n",
    "keywords_to_remove = {\n",
    "    #  Technical Terms (Programming, AI, Cybersecurity, Hardware)\n",
    "    \"algorithm\", \"API\", \"backend\", \"frontend\", \"framework\", \"library\", \"SDK\", \"repository\",\n",
    "    \"debug\", \"deployment\", \"compilation\", \"encryption\", \"decryption\", \"firewall\", \"malware\",\n",
    "    \"cybersecurity\", \"penetration testing\", \"AI\", \"machine learning\", \"deep learning\",\n",
    "    \"neural network\", \"NLP\", \"big data\", \"cloud computing\", \"virtualization\", \"docker\",\n",
    "    \"kubernetes\", \"CI/CD\", \"Git\", \"GitHub\", \"GitLab\", \"bitbucket\", \"blockchain\", \"cryptography\",\n",
    "    \"SQL\", \"NoSQL\", \"database\", \"server\", \"HTTP\", \"HTTPS\", \"REST API\", \"GraphQL\",\n",
    "    \"Linux\", \"Windows\", \"macOS\", \"CLI\", \"shell scripting\", \"command line\", \"bash\",\n",
    "    \"Python\", \"JavaScript\", \"Java\", \"C++\", \"C#\", \"Ruby\", \"Go\", \"Rust\", \"Swift\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"scikit-learn\", \"pandas\", \"NumPy\", \"matplotlib\", \"seaborn\",\n",
    "    \"LSTM\", \"Transformer\", \"GAN\", \"BERT\", \"GPT\", \"LLM\", \"prompt engineering\",\n",
    "    \"latency\", \"bandwidth\", \"packet loss\", \"ping\", \"IP address\", \"VPN\", \"proxy\",\n",
    "    \"router\", \"firmware\", \"BIOS\", \"UEFI\", \"motherboard\", \"GPU\", \"CPU\", \"RAM\", \"cache\",\n",
    "    \"SSD\", \"HDD\", \"NVMe\", \"overclocking\", \"liquid cooling\", \"thermal paste\", \"fan speed\",\n",
    "    \"kernel\", \"driver\", \"firmware update\", \"OpenAI\", \"Google DeepMind\", \"Anthropic\",\n",
    "    \"prompt tuning\", \"fine-tuning\", \"embedding\", \"vector search\", \"data science\",\n",
    "    \n",
    "    #  Gaming Terms (Common in Reddit Discussions)\n",
    "    \"MMORPG\", \"RPG\", \"FPS\", \"MOBA\", \"RTS\", \"battle royale\", \"open world\", \"sandbox\",\n",
    "    \"AAA game\", \"indie game\", \"loot box\", \"DLC\", \"season pass\", \"microtransactions\",\n",
    "    \"PvP\", \"PvE\", \"co-op\", \"multiplayer\", \"single-player\", \"crossplay\", \"level up\",\n",
    "    \"XP\", \"grind\", \"quest\", \"raid\", \"guild\", \"clan\", \"matchmaking\", \"ranked mode\",\n",
    "    \"esports\", \"tournament\", \"pro player\", \"streaming\", \"Twitch\", \"YouTube Gaming\",\n",
    "    \"metagame\", \"nerf\", \"buff\", \"patch\", \"update\", \"hotfix\", \"early access\",\n",
    "    \"beta test\", \"alpha test\", \"game engine\", \"Unreal Engine\", \"Unity\", \"modding\",\n",
    "    \"speedrun\", \"glitch\", \"exploit\", \"cheat code\", \"hacking\", \"aimbot\", \"wallhack\",\n",
    "    \"battle pass\", \"skins\", \"cosmetics\", \"loot system\", \"gacha\", \"AFK\", \"spawn\",\n",
    "    \"respawn\", \"killstreak\", \"headshot\", \"one-shot\", \"combo\", \"stun\", \"cooldown\",\n",
    "    \"mana\", \"HP\", \"MP\", \"boss fight\", \"NPC\", \"cutscene\", \"game physics\", \"hitbox\",\n",
    "    \"frame rate\", \"lag\", \"rubberbanding\", \"ping\", \"dedicated server\", \"netcode\",\n",
    "    \"ray tracing\", \"RTX\", \"DLSS\", \"VSync\", \"FOV\", \"HUD\", \"UI\", \"difficulty curve\",\n",
    "    \"game balancing\", \"open beta\", \"closed beta\", \"Steam\", \"Epic Games\", \"Xbox\", \"PlayStation\",\n",
    "    \"Nintendo Switch\", \"VR gaming\", \"meta\", \"game mechanics\", \"worldbuilding\", \"replayability\",\n",
    "    \"game economy\", \"skill tree\", \"perk system\", \"weapon loadout\", \"battle tactics\",\n",
    "    \"battlefield\", \"arena\", \"ranked ladder\", \"prestige system\", \"game narrative\", \"story mode\"\n",
    "}\n",
    "\n",
    "files2023 = [\"*/2023_02_prepro_train_anonym_V2.tsv\", \"*/2023_02_prepro_train_filtered.tsv\"]\n",
    "files2024 = [\"*/2024_03_prepro_train_anonym.tsv\", \"*/2024_03_prepro_train_filtered.tsv\"]\n",
    "\n",
    "def filter(files, keywords_to_keep, keywords_to_remove):\n",
    "    \"\"\"\n",
    "    Filters lines from an input TSV file and writes them to an output TSV file.\n",
    "    \n",
    "    The line is kept if:\n",
    "      - The content in the 4th column (index 3) contains at least one keyword \n",
    "        from keywords_to_keep.\n",
    "      - The content does not contain any keyword from keywords_to_remove.\n",
    "    \n",
    "    :param files: List of two file paths [input_file_path, output_file_path].\n",
    "    :param keywords_to_keep: A set of keywords (strings) that must appear in the content.\n",
    "    :param keywords_to_remove: A set of keywords (strings) that must not appear in the content.\n",
    "    \"\"\"\n",
    "    input_file, output_file = files\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        \n",
    "        for line in f_in:\n",
    "            # Stripping trailing newlines and split by tab\n",
    "            columns = line.rstrip('\\n').split('\\t')\n",
    "            \n",
    "            # I expect at least 4 columns; skip if there aren't enough\n",
    "            if len(columns) < 4:\n",
    "                continue\n",
    "            \n",
    "            content = columns[3]\n",
    "            \n",
    "            # Checking for at least one good keyword\n",
    "            if not any(good_kw in content for good_kw in keywords_to_keep):\n",
    "                continue\n",
    "            \n",
    "            # Checking for any bad keyword\n",
    "            if any(bad_kw in content for bad_kw in keywords_to_remove):\n",
    "                continue\n",
    "            \n",
    "            # If the line passes the filter, writing it out\n",
    "            f_out.write(line)\n",
    "\n",
    "\n",
    "# Filtering the files\n",
    "print(\"Filtering 2023 file...\")\n",
    "filter(files2023, religious_keywords, keywords_to_remove)\n",
    "print(\"Filtering 2024 file...\")\n",
    "filter(files2024, religious_keywords, keywords_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only filtering out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EROTIC_DATING_KEYWORDS = [\n",
    "    \"sexting\", \"nudes\", \"nude\", \"fetish\", \"creampie\", \"creampied\", \"daddy\", \"pussy\", \"cock\",\n",
    "    \"fuck\", \"horny\", \"snap\", \"onlyfans\", \"nsfw\", \"dm me\", \"sugar daddy\", \"escort\",\n",
    "    \"dominant\", \"submissive\", \"kik\", \"roleplay\", \"erotic\", \"bdsm\", \"anal\", \"blowjob\",\n",
    "    \"handjob\", \"sexy\", \"strip\", \"porn\", \"sex\", \"orgasm\", \"cum\", \"milf\", \"dick\",\n",
    "    \"ass\", \"lick\", \"moan\", \"hot girl\", \"wet\", \"lust\", \"girlfriend\", \"boyfriend\",\n",
    "    \"kiss\", \"dating\", \"hookup\", \"make love\", \"flirt\", \"breast\", \"boobs\", \"tits\", \"fleshlight\", \"tiddy\"\n",
    "]\n",
    "\n",
    "# reading\n",
    "input_file = \"*/combined_classification.tsv\" \n",
    "output_file = \"*/train_combined_filtered.tsv\"\n",
    "\n",
    "# iterating\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        columns = line.strip().split(\"\\t\")  # Splitting into columns\n",
    "\n",
    "        if len(columns) < 5:\n",
    "            outfile.write(line)  \n",
    "            continue\n",
    "\n",
    "        content = columns[3].lower()  \n",
    "        label = columns[-1]  \n",
    "\n",
    "        # Filtering out the keywords\n",
    "        if any(keyword in content for keyword in EROTIC_DATING_KEYWORDS):\n",
    "            columns[-1] = \"0\"  \n",
    "\n",
    "        # Saving\n",
    "        outfile.write(\"\\t\".join(columns) + \"\\n\")\n",
    "\n",
    "print(f\"Filterung abgeschlossen. Datei gespeichert als: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the correctly labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input files\n",
    "input_file1 = \"*/train_part1_corrected.tsv\"  # First TSV file\n",
    "input_file2 = \"*/train_part1_corrected.tsv\"  # Second TSV file\n",
    "input_file3 = \"/Users/laurachristoph/Desktop/Bachelorarbeit/00_10h_binary_classified.tsv\"  # Third TSV file (with author, title, content, etc.)\n",
    "input_file4 = \"/Users/laurachristoph/Desktop/Bachelorarbeit/00_10h_classified_reddit_posts.tsv\"  # Fourth TSV file (title + content merge, category fix)\n",
    "output_file = \"*/train_reliclass.tsv\"  # Output file\n",
    "\n",
    "\n",
    "# Function to copy file line by line (for first two files)\n",
    "def copy_file(file_path, outfile):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            outfile.write(line)  # Writing each line directly to the output file\n",
    "\n",
    "# Opening the output file once to write all processed data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    \n",
    "    # Copying the first two files unchanged\n",
    "    copy_file(input_file1, outfile)\n",
    "    copy_file(input_file2, outfile)\n",
    "\n",
    "    # Determining the correct format based on first file\n",
    "    with open(input_file1, \"r\", encoding=\"utf-8\") as sample_file:\n",
    "        num_columns = len(sample_file.readline().strip().split(\"\\t\"))  # Get correct number of columns\n",
    "\n",
    "    # Function to process structured files (third & fourth)\n",
    "    def process_structured_file(file_path, is_fourth_file=False):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            header = infile.readline().strip().split(\"\\t\")  # Read header\n",
    "\n",
    "            # Getting indices for required fields\n",
    "            title_index = header.index(\"title\") if \"title\" in header else None\n",
    "            content_index = header.index(\"content\") if \"content\" in header else None\n",
    "            label_index = header.index(\"religious\") if \"religious\" in header else \\\n",
    "                          header.index(\"category\") if \"category\" in header else -1  # Last column\n",
    "\n",
    "            # Processing each line\n",
    "            for line in infile:\n",
    "                columns = line.strip().split(\"\\t\")\n",
    "\n",
    "                # Skipping empty or incomplete rows\n",
    "                if len(columns) < max(title_index or 0, content_index or 0, label_index or 0) + 1:\n",
    "                    continue  # Skipping rows with missing columns\n",
    "\n",
    "                # Merging title and content safely\n",
    "                title = columns[title_index] if title_index is not None and title_index < len(columns) else \"\"\n",
    "                content = columns[content_index] if content_index is not None and content_index < len(columns) else \"\"\n",
    "                merged_content = f\"{title} {content}\".strip()\n",
    "\n",
    "                # Adjusting label for fourth file safely\n",
    "                label = \"0\"  # Default label\n",
    "                if label_index != -1 and label_index < len(columns):\n",
    "                    label = columns[label_index].strip().lower()\n",
    "                    if is_fourth_file:\n",
    "                        label = \"1\" if label == \"religious\" else \"0\"  # Converting category labels\n",
    "\n",
    "                # Creating a row with \"None\" values to match the first two files\n",
    "                new_row = [\"None\"] * (num_columns - 2) + [merged_content] + [\"None\"] * (num_columns - len(columns) - 1) + [label]\n",
    "\n",
    "                # Writing the reformatted row directly to the output file\n",
    "                outfile.write(\"\\t\".join(new_row) + \"\\n\")\n",
    "\n",
    "    # Processing the third file\n",
    "    process_structured_file(input_file3)\n",
    "\n",
    "    # Processing the fourth file with category adjustment\n",
    "    process_structured_file(input_file4, is_fourth_file=True)\n",
    "\n",
    "print(f\"File successfully combined and saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of lines in both files\n",
    "def count_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "# Counting the number of lines in the filtered files\n",
    "#lines_2023 = count_lines(files2023[1])\n",
    "#lines_2024 = count_lines(files20*/train_reliclass.tsv24[1])\n",
    "lines_reliclass = count_lines(\"/Users/laurachristoph/Desktop/Bachelorarbeit/00_10h_binary_classified.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of lines in the training file: {lines_reliclass}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model for classifying religious posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels = [sample[\"label\"].item() for sample in dataset]\n",
    "print(Counter(labels))  # Showing class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing out my data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def balance_dataset(input_file, output_file):\n",
    "    religious_count = 0\n",
    "    religious_posts = []\n",
    "    non_religious_posts = []\n",
    "    \n",
    "    # Reading the file line by line\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        header = infile.readline()  # Keeping header if exists\n",
    "        for line in infile:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            if len(columns) < 5:\n",
    "                continue  # Skipping malformed rows\n",
    "            \n",
    "            label = columns[-1].strip()\n",
    "            \n",
    "            if label == \"1\":\n",
    "                religious_count += 1\n",
    "                # Keep 1st and 2nd occurrence, skip 3rd\n",
    "                if religious_count % 3 != 0:\n",
    "                    religious_posts.append(line)\n",
    "            else:\n",
    "                non_religious_posts.append(line)\n",
    "    \n",
    "    # Balancing dataset to have equal religious and non-religious posts\n",
    "    min_class_size = min(len(religious_posts), len(non_religious_posts))\n",
    "    religious_posts = random.sample(religious_posts, min_class_size)\n",
    "    non_religious_posts = random.sample(non_religious_posts, min_class_size)\n",
    "    \n",
    "    # Writing the balanced dataset\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(header)  # Write header back\n",
    "        outfile.writelines(religious_posts + non_religious_posts)\n",
    "    \n",
    "    print(f\"Balanced dataset saved to {output_file}. Final size: {min_class_size * 2} samples.\")\n",
    "\n",
    "input_file = \"*/train_reliclass_balanced2.tsv\"\n",
    "output_file = \"*/train_reliclass_final.tsv\"\n",
    "balance_dataset(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading all needed libraries for training the religious/not-religious classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_libraries():\n",
    "    \"\"\"Installs required libraries for the project.\"\"\"\n",
    "    required_libs = [\n",
    "        \"numpy\", \"pandas\", \"torch\", \"transformers\", \"gensim\",\n",
    "        \"scikit-learn\", \"tqdm\", \"datasets\", \"accelerate\"\n",
    "    ]\n",
    "    \n",
    "    for lib in required_libs:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
    "    \n",
    "    # Installing specific transformer models if not already downloaded\n",
    "    from transformers import DistilBertTokenizer, DistilBertModel\n",
    "    DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    print(\"All necessary libraries are installed and ready to use!\")\n",
    "\n",
    "install_libraries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training on DistilBert\n",
    "Evaluation Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.12      0.22      1339\n",
    "           1       0.53      1.00      0.70      1339\n",
    "\n",
    "    accuracy                           0.56      2678\n",
    "   macro avg       0.77      0.56      0.46      2678\n",
    "weighted avg       0.77      0.56      0.46      2678\n",
    "\n",
    "Test Set Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.13      0.23      1340\n",
    "           1       0.54      1.00      0.70      1339\n",
    "\n",
    "    accuracy                           0.57      2679\n",
    "   macro avg       0.77      0.57      0.47      2679\n",
    "weighted avg       0.77      0.57      0.47      2679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "\n",
    "# Loading DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Function to train DistilBERT model\n",
    "def train_distilbert(X_train, y_train, X_eval, y_eval, save_path):\n",
    "    \"\"\"Trains a DistilBERT model and evaluates it.\"\"\"\n",
    "    \n",
    "    # Converting data to Hugging Face Dataset format\n",
    "    train_dataset = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
    "    eval_dataset = Dataset.from_dict({\"text\": X_eval, \"label\": y_eval})\n",
    "    \n",
    "    # Tokenizing datasets\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Loading pre-trained DistilBERT model for classification\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    \n",
    "    # Defining training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,  # Train for more epochs\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Training the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Saving the best model\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Best DistilBERT model saved to: {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "save_path = \"*/best_distilbert_model\"\n",
    "best_distilbert_model = train_distilbert(X_train, y_train, X_eval, y_eval, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training on RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to install missing dependencies\n",
    "def install_packages():\n",
    "    packages = [\"transformers[torch]\", \"torch\"]\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.split(\"[\")[0])  # Checking if package is installed\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-U\"])\n",
    "\n",
    "# Installing missing packages\n",
    "install_packages()\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Loading RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Function to load data from a tab-separated text file\n",
    "def load_data(file_path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue  # Skipping malformed lines\n",
    "            texts.append(parts[-2])  # Assuming second last column is text\n",
    "            labels.append(int(parts[-1]))  # Assuming last column is the label\n",
    "    return texts, labels\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Function to initialize weights\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "# Function to train RoBERTa model manually\n",
    "def train_roberta(train_file, eval_file, save_path, epochs=30, batch_size=16, lr=2e-5):\n",
    "    \"\"\"Trains a RoBERTa model and evaluates it\"\"\"\n",
    "    \n",
    "    # Load and tokenize data\n",
    "    X_train, y_train = load_data(train_file)\n",
    "    X_eval, y_eval = load_data(eval_file)\n",
    "    \n",
    "    train_encodings = tokenize_texts(X_train)\n",
    "    eval_encodings = tokenize_texts(X_eval)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "    eval_labels = torch.tensor(y_eval, dtype=torch.long)\n",
    "    \n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "    eval_dataset = TensorDataset(eval_encodings['input_ids'], eval_encodings['attention_mask'], eval_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Load pre-trained RoBERTa model for classification\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "    model.apply(initialize_weights)  # Initialize classifier weights properly\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer, loss function, and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    class_weights = torch.tensor([1.0, 1.0]).to(device)  # Adjust for imbalance if needed\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_loader) * epochs,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(np.array(all_preds), np.array(all_labels))\n",
    "    print(f\"Evaluation Metrics: {metrics}\")\n",
    "    \n",
    "    # Saving the best model\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Best RoBERTa model saved to: {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "train_file = \"*/train_reliclass_train_new.tsv\"\n",
    "eval_file = \"*/train_reliclass_eval_new.tsv\"\n",
    "save_path = \"*/best_roberta_model\"\n",
    "\n",
    "best_roberta_model = train_roberta(train_file, eval_file, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Analyze the issue by analyzing the misclassified or low-confidence posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Setting file paths\n",
    "model_path = \"*/best_roberta_model\"\n",
    "train_path = \"*/train_reliclass_train_new.tsv\"\n",
    "eval_path = \"*/train_reliclass_eval_new.tsv\"\n",
    "\n",
    "# Loading tokenizer & model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # Setting model to evaluation mode\n",
    "\n",
    "# Function to read TSV data and combine train + eval\n",
    "def load_combined_data(train_file, eval_file):\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for file_path in [train_file, eval_file]:  # Process both files\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) < 18:\n",
    "                    continue  # Skip malformed rows\n",
    "\n",
    "                text = parts[3]  # Assuming text is in column 4\n",
    "                label = parts[-1]  # Assuming label is in column 18\n",
    "\n",
    "                if label in {\"0\", \"1\"}:  # Ensure valid labels\n",
    "                    texts.append(text)\n",
    "                    labels.append(int(label))\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Loading and combine train + eval\n",
    "X_data, y_data = load_combined_data(train_path, eval_path)\n",
    "\n",
    "# Function to classify texts\n",
    "def classify_texts(texts):\n",
    "    predictions, confidences = [], []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze().tolist()  # Converting logits to probabilities\n",
    "\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "        predictions.append(predicted_class)\n",
    "        confidences.append(probs[predicted_class])  # Storing confidence of predicted class\n",
    "\n",
    "    return predictions, confidences\n",
    "\n",
    "# Running classification on full dataset\n",
    "y_pred, confidences = classify_texts(X_data)\n",
    "\n",
    "# Identifying misclassified, low-confidence, and correctly classified confident examples\n",
    "misclassified = []\n",
    "low_confidence = []\n",
    "correct_confident = []\n",
    "\n",
    "threshold = 0.65  # Confidence threshold\n",
    "\n",
    "for i, (text, true_label, pred_label, conf) in enumerate(zip(X_data, y_data, y_pred, confidences)):\n",
    "    if pred_label != true_label:\n",
    "        misclassified.append((text, true_label, pred_label, conf))\n",
    "    elif conf < threshold:\n",
    "        low_confidence.append((text, true_label, pred_label, conf))\n",
    "    else:\n",
    "        correct_confident.append((text, true_label, pred_label, conf))\n",
    "\n",
    "# Printing misclassified examples\n",
    "print(\"\\nMisclassified Examples:\")\n",
    "for i, (text, true, pred, conf) in enumerate(misclassified[:10]):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true} | Predicted Label: {pred} | Confidence: {conf:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Misclassified: {len(misclassified)}\")\n",
    "\n",
    "# Printing low-confidence examples\n",
    "print(\"\\nLow-Confidence Examples:\")\n",
    "for i, (text, true, pred, conf) in enumerate(low_confidence[:10]):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true} | Predicted Label: {pred} | Confidence: {conf:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Low-Confidence: {len(low_confidence)}\")\n",
    "\n",
    "# Printing correctly classified & confident examples\n",
    "print(\"\\nCorrect & Confident Examples:\")\n",
    "for i, (text, true, pred, conf) in enumerate(correct_confident[:10]):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true} | Predicted Label: {pred} | Confidence: {conf:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Correct & Confident: {len(correct_confident)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Downloading the misclassified and low confidence posts to reannotate them\n",
    "Noticed that some are misclassified by me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is the final religion classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Function to install missing dependencies\n",
    "def install_packages():\n",
    "    packages = [\"transformers[torch]\", \"torch\"]\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.split(\"[\")[0])  # Checking if package is installed\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-U\"])\n",
    "\n",
    "# Installing missing packages\n",
    "install_packages()\n",
    "\n",
    "# Loading RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Function to initialize model weights\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "# Function to train RoBERTa model with early stopping\n",
    "def train_roberta(content, labels, save_path, epochs=30, batch_size=16, lr=2e-5, patience=5):\n",
    "    \"\"\"Trains a RoBERTa model using the provided content and labels with early stopping.\"\"\"\n",
    "\n",
    "    # Shuffling and split data (80% train, 20% eval)\n",
    "    dataset = list(zip(content, labels))\n",
    "    random.shuffle(dataset)\n",
    "    split_idx = int(0.8 * len(dataset))\n",
    "    train_data, eval_data = dataset[:split_idx], dataset[split_idx:]\n",
    "\n",
    "    # Separating text and labels\n",
    "    X_train, y_train = zip(*train_data)\n",
    "    X_eval, y_eval = zip(*eval_data)\n",
    "\n",
    "    # Tokenizing data\n",
    "    train_encodings = tokenize_texts(list(X_train))\n",
    "    eval_encodings = tokenize_texts(list(X_eval))\n",
    "\n",
    "    # Converting to PyTorch tensors\n",
    "    train_labels = torch.tensor(y_train, dtype=torch.long)\n",
    "    eval_labels = torch.tensor(y_eval, dtype=torch.long)\n",
    "\n",
    "    # Preparing DataLoader\n",
    "    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "    eval_dataset = TensorDataset(eval_encodings['input_ids'], eval_encodings['attention_mask'], eval_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Loading pre-trained RoBERTa model for classification\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "    model.apply(initialize_weights)  # Initialize classifier weights\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Defining optimizer, loss function, and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    class_weights = torch.tensor([1.0, 1.0]).to(device)  # Adjust for imbalance if needed\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_loader) * epochs,\n",
    "    )\n",
    "\n",
    "    # Early Stopping Parameters\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_train_loss:.5f}\")\n",
    "\n",
    "        # Evaluating after each epoch\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        eval_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_loader:\n",
    "                input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = outputs.logits.cpu().numpy()\n",
    "                eval_loss += loss_fn(outputs.logits, labels).item()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_eval_loss = eval_loss / len(eval_loader)\n",
    "        metrics = compute_metrics(np.array(all_preds), np.array(all_labels))\n",
    "        print(f\"Evaluation Metrics: {metrics}\")\n",
    "\n",
    "        # Checking early stopping\n",
    "        if avg_eval_loss < best_loss:\n",
    "            best_loss = avg_eval_loss\n",
    "            epochs_no_improve = 0\n",
    "            print(\"Model improved, saving checkpoint.\")\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epochs.\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break  # Stop training if no improvement for `patience` epochs\n",
    "\n",
    "    print(f\" Training complete. Best model saved to: {save_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Defining save path for model\n",
    "save_path = \"*/best_roberta_model_renovated\"\n",
    "\n",
    "# Training the RoBERTa model with content & labels\n",
    "best_roberta_model = train_roberta(content, labels, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to distill the RoBERTa model because it is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Getting device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Loading tokenizer, teacher, and student models\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "teacher = RobertaForSequenceClassification.from_pretrained(\"*/best_roberta_model_renovated\").to(device)\n",
    "student = RobertaForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=2).to(device)\n",
    "teacher.eval()  # freeze teacher\n",
    "\n",
    "# Distillation loss function\n",
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha=0.5):\n",
    "    soft_teacher = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
    "    soft_student = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
    "    distill_loss = nn.KLDivLoss(reduction='batchmean')(soft_student, soft_teacher) * (temperature ** 2)\n",
    "    ce_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    return alpha * distill_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "# Distillation function\n",
    "def distill_from_file(input_tsv, output_dir, chunk_size=10000, batch_size=16, epochs=1, temperature=2.0, alpha=0.5):\n",
    "    optimizer = optim.AdamW(student.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        chunk_iter = pd.read_csv(input_tsv, sep=\"\\t\", chunksize=chunk_size)\n",
    "\n",
    "        for chunk_id, chunk in enumerate(chunk_iter):\n",
    "            # Dropping rows with missing content or label\n",
    "            chunk = chunk.dropna(subset=[\"content\", \"label\"])\n",
    "            texts = chunk[\"content\"].astype(str).tolist()\n",
    "            labels = torch.tensor(chunk[\"label\"].tolist(), dtype=torch.long)\n",
    "\n",
    "            # Tokenizing all inputs\n",
    "            encodings = tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "            input_ids = encodings[\"input_ids\"]\n",
    "            attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "            # Batching training\n",
    "            num_batches = len(input_ids) // batch_size + int(len(input_ids) % batch_size != 0)\n",
    "            for i in tqdm(range(num_batches), desc=f\"Chunk {chunk_id + 1}\"):\n",
    "                b_input_ids = input_ids[i*batch_size:(i+1)*batch_size].to(device)\n",
    "                b_attention = attention_mask[i*batch_size:(i+1)*batch_size].to(device)\n",
    "                b_labels = labels[i*batch_size:(i+1)*batch_size].to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher(b_input_ids, attention_mask=b_attention).logits\n",
    "\n",
    "                student.train()\n",
    "                student_logits = student(b_input_ids, attention_mask=b_attention).logits\n",
    "                loss = distillation_loss(student_logits, teacher_logits, b_labels, temperature, alpha)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    # Saving student model\n",
    "    student.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"\\n Distilled model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my paths and settings\n",
    "input_tsv = \"*/train_inspected_corrected.tsv\"\n",
    "output_path = \"mini_reli_classifier\"\n",
    "label_column = \"label\"  \n",
    "\n",
    "distill_from_file(\n",
    "    input_tsv=input_tsv,\n",
    "    output_dir=output_path,\n",
    "    chunk_size=15000,   \n",
    "    batch_size=16,\n",
    "    epochs=2,           \n",
    "    temperature=2.0,\n",
    "    alpha=0.5           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Random Forest model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_eval, y_eval, X_test, y_test, save_path):\n",
    "    \"\"\"Trains a Random Forest model with hyperparameter tuning and evaluates it.\"\"\"\n",
    "    \n",
    "    # Defining hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 300, 500],  # Number of trees\n",
    "        \"max_depth\": [10, 20, None],  # Depth of trees\n",
    "        \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n",
    "        \"min_samples_leaf\": [1, 5, 10],  # Minimum samples in a leaf\n",
    "        \"class_weight\": [\"balanced_subsample\"],  # Handle class imbalance\n",
    "    }\n",
    "\n",
    "    # Initializing Random Forest model\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=300,  # More trees help generalization\n",
    "        max_depth=20,  # Limiting tree depth\n",
    "        min_samples_leaf=5,  # Prevents deep overfitting trees\n",
    "        class_weight={0: 2, 1: 1},  # Balance classes\n",
    "        random_state=42\n",
    "                                )\n",
    "\n",
    "    # Performing GridSearch to find the best parameters\n",
    "    grid_search = GridSearchCV(rf_clf, param_grid, cv=5, scoring=\"f1\", n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Getting the best model\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluating on evaluation set\n",
    "    y_pred_eval = best_rf_model.predict(X_eval)\n",
    "    print(\"Evaluation Set Performance:\")\n",
    "    print(classification_report(y_eval, y_pred_eval))\n",
    "\n",
    "    # Evaluating on test set\n",
    "    y_pred_test = best_rf_model.predict(X_test)\n",
    "    print(\"Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "    # Saving the best trained model\n",
    "    import joblib\n",
    "    joblib.dump(best_rf_model, save_path)\n",
    "    print(f\"Best Random Forest model saved to: {save_path}\")\n",
    "\n",
    "    return best_rf_model\n",
    "\n",
    "\n",
    "save_path = \"*/best_random_forest_model.pkl\"\n",
    "best_random_forest = train_random_forest(X_train, y_train, X_eval, y_eval, X_test, y_test, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the whole anonymized training data through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        return f\"{int(seconds)} sec\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{int(seconds // 60)} min {int(seconds % 60)} sec\"\n",
    "    else:\n",
    "        return f\"{int(seconds // 3600)} hr {int((seconds % 3600) // 60)} min\"\n",
    "\n",
    "def classify_religious_content_from_tsv(input_file, output_file, model_path, batch_size=16, chunk_size=15000, total_lines=49856969):\n",
    "    \"\"\"\n",
    "    Predicts labels for a large TSV file, using the combined 'title' and 'selftext' fields.\n",
    "    Writes results to disk incrementally and estimates total time.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    first_chunk = True\n",
    "    total_chunks = total_lines // chunk_size + int(total_lines % chunk_size != 0)\n",
    "    chunk_idx = 0\n",
    "    total_elapsed = 0\n",
    "\n",
    "    for chunk in pd.read_csv(input_file, sep=\"\\t\", chunksize=chunk_size):\n",
    "        start_time = time.time()\n",
    "\n",
    "        chunk[\"title\"] = chunk[\"title\"].fillna(\"\")\n",
    "        chunk[\"selftext\"] = chunk[\"selftext\"].fillna(\"\")\n",
    "        texts = (chunk[\"title\"] + \" \" + chunk[\"selftext\"]).tolist()\n",
    "        all_preds = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Batching Chunk {chunk_idx+1}/{total_chunks}\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().tolist()\n",
    "                all_preds.extend(preds)\n",
    "\n",
    "        chunk[\"predicted_label\"] = all_preds\n",
    "        chunk.to_csv(output_file, sep=\"\\t\", index=False, mode='a' if not first_chunk else 'w', header=first_chunk)\n",
    "        first_chunk = False\n",
    "\n",
    "        # Time estimation\n",
    "        elapsed = time.time() - start_time\n",
    "        total_elapsed += elapsed\n",
    "        chunk_idx += 1\n",
    "        chunks_left = total_chunks - chunk_idx\n",
    "        avg_time_per_chunk = total_elapsed / chunk_idx\n",
    "        est_remaining = avg_time_per_chunk * chunks_left\n",
    "\n",
    "        print(f\" Chunk {chunk_idx}/{total_chunks} complete. Time: {format_time(elapsed)}. Estimated remaining: {format_time(est_remaining)}.\")\n",
    "\n",
    "    print(f\"\\nAll done! Total time: {format_time(total_elapsed)}. Predictions saved to: {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "input_file = \"*/cleaned_dataset.tsv\"\n",
    "output_file = \"*/complete_relipred_miniclassifier.tsv\"\n",
    "model_path = \"mini_reli_classifier\"\n",
    "\n",
    "classify_religious_content_from_tsv(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    model_path=model_path,\n",
    "    batch_size=16,\n",
    "    chunk_size=15000,\n",
    "    total_lines=49856969\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_predict_religious import run_parallel_prediction\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "run_parallel_prediction(\n",
    "    input_file=\"*/cleaned_dataset.tsv\",\n",
    "    model_path=\"mini_reli_classifier\",\n",
    "    output_dir=\"*/predicted_chunks\",\n",
    "    chunk_size=15000,\n",
    "    batch_size=32,\n",
    "    num_workers=4  \n",
    ")\n",
    "\n",
    "files = sorted(glob.glob(\"*/predicted_chunks/chunk_*.tsv\"))\n",
    "dfs = [pd.read_csv(f, sep=\"\\t\") for f in files]\n",
    "pd.concat(dfs).to_csv(\"*/complete_relipred_miniclassifier.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Getting a sorted list of all chunk file paths\n",
    "files = sorted(glob.glob(\"*/predicted_chunks/chunk_*.tsv\"))\n",
    "\n",
    "# Reading each file \n",
    "dfs = [pd.read_csv(f, sep=\"\\t\") for f in files]\n",
    "\n",
    "# Concatenating all DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Saving the merged DataFrame to a new TSV file with a single header\n",
    "merged_df.to_csv(\"*/merged_predicted_chunks.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Merged file saved as '*/merged_reli_chunks_2603.tsv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
