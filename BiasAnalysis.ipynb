{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Load and Merge Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the folder path where my TSV/TXT files are stored\n",
    "folder_path = \"*/LLMs_Stance and Category\"\n",
    "\n",
    "# Load all model files into a dictionary\n",
    "all_models = {}\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".tsv\") or file.endswith(\".txt\"):\n",
    "        # Clean the model name\n",
    "        raw_name = os.path.splitext(file)[0]\n",
    "        model_name = re.sub(r\"[_ ]?Stance[_ ]?and[_ ]?Category.*\", \"\", raw_name, flags=re.IGNORECASE)\n",
    "\n",
    "        # Load the data\n",
    "        df = pd.read_csv(os.path.join(folder_path, file), sep=\"\\t\", header=None)\n",
    "        df.columns = [\"text\", f\"{model_name}_stance\", f\"{model_name}_category\"]\n",
    "\n",
    "        all_models[model_name] = df\n",
    "\n",
    "# Merge all on the 'text' column\n",
    "merged_df = list(all_models.values())[0]\n",
    "for df in list(all_models.values())[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on=\"text\", how=\"inner\")\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Add Ground Truth Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_path = \"*/golden_standard.txt\"\n",
    "# Loading the ground truth labels\n",
    "truth_df = pd.read_csv(truth_path, sep=\"\\t\", header=None, usecols=[0, 1, 2], encoding=\"latin1\")\n",
    "# Renaming columns for clarity\n",
    "truth_df.columns = [\"text\", \"true_stance\", \"true_category\"]\n",
    "\n",
    "\n",
    "# Merging with the combined model predictions\n",
    "final_df = pd.merge(truth_df, merged_df, on=\"text\", how=\"inner\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def is_correct(y_true, y_pred):\n",
    "    return [\"correct\" if yt in yp else \"incorrect\" for yt, yp in zip(y_true, y_pred)]\n",
    "\n",
    "model_names = [name for name in all_models.keys()]\n",
    "\n",
    "for model in model_names:\n",
    "    print(f\"\\n=== {model} ===\")\n",
    "    \n",
    "    # STANCE\n",
    "    stance_true = final_df[\"true_stance\"].astype(str)\n",
    "    stance_pred = final_df[f\"{model}_stance\"].astype(str)\n",
    "    stance_eval = is_correct(stance_true, stance_pred)\n",
    "\n",
    "    print(\"STANCE REPORT (contains match):\")\n",
    "    print(classification_report(stance_eval, [\"correct\"] * len(stance_eval), zero_division=0))\n",
    "\n",
    "    # CATEGORY\n",
    "    category_true = final_df[\"true_category\"].astype(str)\n",
    "    category_pred = final_df[f\"{model}_category\"].astype(str)\n",
    "    category_eval = is_correct(category_true, category_pred)\n",
    "\n",
    "    print(\"CATEGORY REPORT (contains match):\")\n",
    "    print(classification_report(category_eval, [\"correct\"] * len(category_eval), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def is_correct(y_true, y_pred):\n",
    "    return [\"correct\" if yt in yp else \"incorrect\" for yt, yp in zip(y_true, y_pred)]\n",
    "\n",
    "# Collecting results\n",
    "results = []\n",
    "\n",
    "for model in all_models.keys():\n",
    "    stance_true = final_df[\"true_stance\"].astype(str)\n",
    "    stance_pred = final_df[f\"{model}_stance\"].astype(str)\n",
    "    stance_eval = is_correct(stance_true, stance_pred)\n",
    "\n",
    "    category_true = final_df[\"true_category\"].astype(str)\n",
    "    category_pred = final_df[f\"{model}_category\"].astype(str)\n",
    "    category_eval = is_correct(category_true, category_pred)\n",
    "\n",
    "    stance_f1 = f1_score([\"correct\"] * len(stance_eval), stance_eval, pos_label=\"correct\", zero_division=0)\n",
    "    category_f1 = f1_score([\"correct\"] * len(category_eval), category_eval, pos_label=\"correct\", zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model,\n",
    "        \"stance_f1\": stance_f1,\n",
    "        \"category_f1\": category_f1\n",
    "    })\n",
    "\n",
    "# Making it a DataFram\n",
    "f1_df = pd.DataFrame(results)\n",
    "\n",
    "# Simplifying the model names\n",
    "f1_df[\"model\"] = f1_df[\"model\"].str.replace(r\"[^a-zA-Z0-9\\-]+\", \"_\", regex=True)\n",
    "\n",
    "# Saving files\n",
    "output_path = \"*/f1_comparison_results.tsv\"\n",
    "f1_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"F1-Vergleich gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading F1 results\n",
    "f1_df = pd.read_csv(\"*/f1_comparison_results.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Sorting by average F1 score (optional)\n",
    "f1_df[\"avg_f1\"] = (f1_df[\"stance_f1\"] + f1_df[\"category_f1\"]) / 2\n",
    "f1_df = f1_df.sort_values(\"avg_f1\", ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(f1_df))\n",
    "\n",
    "plt.bar([i - 0.2 for i in x], f1_df[\"stance_f1\"], width=0.4, label=\"Stance F1\")\n",
    "plt.bar([i + 0.2 for i in x], f1_df[\"category_f1\"], width=0.4, label=\"Category F1\")\n",
    "\n",
    "plt.xticks(ticks=x, labels=f1_df[\"model\"], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Model Comparison: F1 Scores for Stance and Category\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Saving plot\n",
    "plot_path = \"*/f1_score_comparison_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved at: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(true, pred, title):\n",
    "    labels = sorted(list(set(true) | set(pred)))\n",
    "    cm = confusion_matrix(true, pred, labels=labels)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_conf_matrix(final_df[\"true_stance\"], final_df[f\"{model_names[0]}_stance\"], f\"{model_names[0]} - Stance Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "STANCE_CHOICES = [\n",
    "    \"anti-Christianity\", \"anti-Islam\", \"anti-Hinduism\",\n",
    "    \"anti-Buddhism\", \"anti-Atheism\", \"undefined\"\n",
    "]\n",
    "\n",
    "CATEGORY_CHOICES = [\n",
    "    \"implicit animosity\", \"explicit derogation\", \"dehumanization\",\n",
    "    \"threatening language\", \"abusive humor\", \"undefined\"\n",
    "]\n",
    "\n",
    "def compute_stance_distribution(series, label_set):\n",
    "    counts = defaultdict(float)\n",
    "    total = 0\n",
    "    for entry in series.dropna().astype(str):\n",
    "        labels = [label.strip() for label in entry.split(\",\") if label.strip()]\n",
    "        weight = 1.0 / len(labels)\n",
    "        for label in labels:\n",
    "            if label in label_set:\n",
    "                counts[label] += weight\n",
    "        total += 1\n",
    "    return {label: counts[label] / total for label in label_set}\n",
    "\n",
    "def compute_category_distribution(series, label_set):\n",
    "    counts = defaultdict(float)\n",
    "    for entry in series.dropna().astype(str):\n",
    "        labels = [label.strip() for label in entry.split(\",\") if label.strip()]\n",
    "        weight = 1.0 / len(labels)\n",
    "        for label in labels:\n",
    "            if label in label_set:\n",
    "                counts[label] += weight\n",
    "    total = sum(counts.values())\n",
    "    return {label: counts[label] / total if total > 0 else 0 for label in label_set}\n",
    "\n",
    "# Computing all distributions\n",
    "stance_data = {}\n",
    "category_data = {}\n",
    "\n",
    "for model in model_names:\n",
    "    stance_data[model] = compute_stance_distribution(final_df[f\"{model}_stance\"], STANCE_CHOICES)\n",
    "    category_data[model] = compute_category_distribution(final_df[f\"{model}_category\"], CATEGORY_CHOICES)\n",
    "\n",
    "stance_df = pd.DataFrame.from_dict(stance_data, orient=\"index\")[STANCE_CHOICES]\n",
    "category_df = pd.DataFrame.from_dict(category_data, orient=\"index\")[CATEGORY_CHOICES]\n",
    "\n",
    "# ========== 1. GROUPED BAR PLOT (STANCE DISTRIBUTION) ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "stance_df_plot = stance_df.T\n",
    "x = range(len(STANCE_CHOICES))\n",
    "bar_width = 0.8 / len(stance_df_plot.columns)\n",
    "\n",
    "for i, model in enumerate(stance_df_plot.columns):\n",
    "    plt.bar(\n",
    "        [xi + i * bar_width for xi in x],\n",
    "        stance_df_plot[model],\n",
    "        width=bar_width,\n",
    "        label=model\n",
    "    )\n",
    "\n",
    "plt.xticks([xi + bar_width * (len(stance_df_plot.columns) / 2) for xi in x], STANCE_CHOICES, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Stance Distribution by Model\")\n",
    "# Setting legend below the plot\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.25), ncol=4)\n",
    "\n",
    "# Manually adjusting the bottom to make space for legend\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.savefig(\"stance_grouped_bar.png\")\n",
    "plt.show()\n",
    "\n",
    "# ========== 2. STACKED BAR PLOT (CATEGORY DISTRIBUTION) ==========\n",
    "plt.figure(figsize=(12, 6))\n",
    "bottoms = [0] * len(category_df)\n",
    "\n",
    "for category in CATEGORY_CHOICES:\n",
    "    heights = category_df[category]\n",
    "    plt.bar(category_df.index, heights, bottom=bottoms, label=category)\n",
    "    bottoms = [b + h for b, h in zip(bottoms, heights)]\n",
    "\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Category Distribution by Model (Stacked)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "# Setting legend below the plot\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.25), ncol=4)\n",
    "\n",
    "# Manually adjusting the bottom to make space for legend\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.savefig(\"category_stacked_bar.png\")\n",
    "plt.show()\n",
    "\n",
    "# ========== 3. HEATMAP (CATEGORY DISTRIBUTION) ==========\n",
    "plt.figure(figsize=(10, len(category_df) * 0.5 + 3))\n",
    "sns.heatmap(category_df, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar_kws={'label': 'Proportion'})\n",
    "plt.title(\"Category Distribution per Model (Heatmap)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"category_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"All visualizations saved as:\")\n",
    "print(\"- stance_grouped_bar.png\")\n",
    "print(\"- category_stacked_bar.png\")\n",
    "print(\"- category_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Model Disagreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Pairwise Model Disagreements (Stance with partial match) ===\")\n",
    "\n",
    "for model1, model2 in combinations(model_names, 2):\n",
    "    stance1 = final_df[f\"{model1}_stance\"].astype(str)\n",
    "    stance2 = final_df[f\"{model2}_stance\"].astype(str)\n",
    "\n",
    "    agreement = [\n",
    "        (s1 in s2) or (s2 in s1) for s1, s2 in zip(stance1, stance2)\n",
    "    ]\n",
    "\n",
    "    total = len(stance1)\n",
    "    disagreement_count = total - sum(agreement)\n",
    "    print(f\"{model1} vs {model2}: {disagreement_count}/{total} ({disagreement_count / total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty DataFrame\n",
    "disagreement_matrix = pd.DataFrame(index=model_names, columns=model_names, dtype=float)\n",
    "\n",
    "# Computing pairwise disagreement rates\n",
    "for model1, model2 in combinations(model_names, 2):\n",
    "    stance1 = final_df[f\"{model1}_stance\"].astype(str)\n",
    "    stance2 = final_df[f\"{model2}_stance\"].astype(str)\n",
    "\n",
    "    agreement = [\n",
    "        (s1 in s2) or (s2 in s1) for s1, s2 in zip(stance1, stance2)\n",
    "    ]\n",
    "    disagreement = 1 - (sum(agreement) / len(agreement))\n",
    "    \n",
    "    # Filling both [model1][model2] and [model2][model1]\n",
    "    disagreement_matrix.loc[model1, model2] = disagreement\n",
    "    disagreement_matrix.loc[model2, model1] = disagreement\n",
    "\n",
    "# Filling diagonal with 0 (no disagreement with self)\n",
    "for model in model_names:\n",
    "    disagreement_matrix.loc[model, model] = 0.0\n",
    "\n",
    "# Plotting heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(disagreement_matrix.astype(float), annot=True, fmt=\".2%\", cmap=\"Reds\", square=True,\n",
    "            cbar_kws={\"label\": \"Disagreement Rate\"}, linewidths=0.5, linecolor=\"gray\")\n",
    "plt.title(\"Pairwise Stance Disagreement Between Models\\n(Partial Match Considered Agreement)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pairwise_stance_disagreement_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Disagreement heatmap saved as: pairwise_stance_disagreement_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Inspect Disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows with disagreements between two example models\n",
    "disagreements = final_df[\n",
    "    (final_df[f\"{model_names[0]}_stance\"] != final_df[f\"{model_names[1]}_stance\"]) |\n",
    "    (final_df[f\"{model_names[0]}_category\"] != final_df[f\"{model_names[1]}_category\"])\n",
    "]\n",
    "\n",
    "# Showing sample rows\n",
    "disagreements[['text', 'true_stance', f\"{model_names[0]}_stance\", f\"{model_names[1]}_stance\",\n",
    "               'true_category', f\"{model_names[0]}_category\", f\"{model_names[1]}_category\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prining all columns to debug\n",
    "print(\"All columns in DataFrame:\")\n",
    "print(final_df.columns.tolist())\n",
    "\n",
    "# Identifying model stance/category columns\n",
    "stance_cols = [col for col in final_df.columns if col.endswith('_stance') and not col.startswith('true_')]\n",
    "category_cols = [col for col in final_df.columns if col.endswith('_category') and not col.startswith('true_')]\n",
    "\n",
    "print(\"\\nDetected stance columns:\", stance_cols)\n",
    "print(\"Detected category columns:\", category_cols)\n",
    "\n",
    "# Function to check if all values in a row across given columns are the same\n",
    "def not_all_equal(row, cols):\n",
    "    values = row[cols].tolist()\n",
    "    return len(set(values)) > 1\n",
    "\n",
    "# Applying filter\n",
    "filtered_df = final_df[final_df.apply(lambda row: not_all_equal(row, stance_cols) or not_all_equal(row, category_cols), axis=1)]\n",
    "\n",
    "# Dropping true_* columns \n",
    "for col in ['true_stance', 'true_category']:\n",
    "    if col in filtered_df.columns:\n",
    "        filtered_df = filtered_df.drop(columns=[col])\n",
    "\n",
    "# Saving to TSV\n",
    "filtered_df.to_csv('*/disagreements.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
